= Demo for Consuming from a Kafka Topic in Confluent Cloud with an AWS Lambda

This demonstrates how to consuming from a Kafka Topic in Confluent Cloud with an AWS Lambda.

DISCLAIMER: This project is for demonstration purposes only. Using the demo unmodified in production is highly discouraged. Use at your own risk.

== Building the Lambda

Go to subfolder `java` and build the the Lamda code:

```shell
./gradlew build
```

In some cases you might need to rebuild the Gradle wrapper first:

```shell
gradle wrapper
```

You should now have zip file containing the compiled Lambda in `java/app/build/distributions/app.zip`.

== Setting up the Confluent Cloud and AWS infrastrucure

First, create a Confluent Cloud API key which has permission to create Kafka clusters (e.g. OrganizationAdmin). If you do not have this access permissions, you need to update the Terraform configuration accordingly and use pre-existing resources instead. At the minimum, you need to have a basic Kafka cluster with (in this example) public endpoint, a topic, two service acocunts (SAs), i.e. one SA for producing events, one SA for consuming them (the latter will be used for the Lambda) with approriate access permissions, and API keys for both service accounts.

In the following, we assume that you have permission to create a cluster and all other resources in Confluent.

Place the file `api-key.txt` containing your Confluent Cloud API Key (the one with OrganizationAdmin) in the subfolder `terraform/java`. Alternatively, you can also use the following enviroment variables:

```shell
export CONFLUENT_CLOUD_API_KEY="<ID-OF-YOUR-API-KEY>" 
export CONFLUENT_CLOUD_API_SECRET="<SECRET-OF-YOUR-API-KEY>"
```

Make sure you have logged into AWS and sufficient permission to create the required resources. You can check that you are logged in for example by calling:

```shell
aws sts get-caller-identity
```

Now set/customize some variables by copying `terraform.tfvars.template` to `terraform.tfvars`. Set at least the environment ID (in this demo we assume to use an existing environment).

If you start from scratch, initialize terraform once:

```shell
terraform init
```

Check the plan generated by terraform:

```shell
terraform plan
```

If you agree with the plan, deploy the resources (that will take some time):

```shell
terraform apply
```

== Testing the setup

In your browser, go to the AWS Lambda overview. You should see your newly created Lambda. In the `monitor` section you find a link to CloudWatch.

You can now produce sample data with the Kafka console producer. Just go to subfolder `terraform/java/generated/client-configs` and run console producer (you can find the whole command line in `client-producer.conf` if you haven't disabled this feature in the variables):

```shell
kafka-console-producer --producer.config client-producer.conf --bootstrap-server <bootstrap-server-with-port> --topic test
```

You can now paste the content of `examples/example-json-event.json` as a single line to the producer. Of course, you can customize the data.

After a while, you should see a entry in Cloud Watch.


== Wrapping things up

You can destroy all created resources including the cluster in Confluent Cloud by running the following command:

```shell
terraform destroy
```
